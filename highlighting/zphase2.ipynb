{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c3adcbbfcf4cfd8abacf07ecc5ab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math \n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from highlight import *\n",
    "from highlight import XentLang as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "model.save_pretrained(\"models/gpt2-xl-M0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d962ef12ba04534ac8edd87594e5703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"models/gpt2-xl-M0/\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/gpt2-xl-M0/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    task_manager = HilightWiki()\n",
    "    textok, t1, t2 = task_manager.extract()\n",
    "\n",
    "    min_y_window = int(sub_window_size)\n",
    "    i = random.randint(0, textok.size(-1) - min_y_window)\n",
    "    y = textok[0][i:i+min_y_window]\n",
    "\n",
    "    hilisorted = task_manager.xent_sort(y.unsqueeze(0), t1, t2)\n",
    "\n",
    "    response = detokenize_l(hilisorted)\n",
    "\n",
    "    keep_best = 20\n",
    "\n",
    "    red_best = X().comma.join(response[:keep_best])\n",
    "    blue_best = X().comma.join(response[-keep_best:])\n",
    "\n",
    "    min_y_window = int(sub_window_size)\n",
    "\n",
    "    i = random.randint(0, textok.size(-1) - min_y_window)\n",
    "    slice = textok[0][i:i+min_y_window]\n",
    "    ywidth = 3\n",
    "    o = slice[:ywidth]\n",
    "    c = slice[-ywidth:]\n",
    "\n",
    "    y = (detokenize_w(o), detokenize_w(c))\n",
    "    # print(y)\n",
    "    x1 = detokenize_0(t1)\n",
    "    x2 = detokenize_0(t2)\n",
    "\n",
    "    text = detokenize_0(textok)\n",
    "    func = redblue(y, x1, x2)\n",
    "\n",
    "    output = text+\"\\n\"+func+\"\\n\"+red_best+X().comma+blue_best\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "pyinstrument ........................................\n",
      ".\n",
      ".  Block at /tmp/ipykernel_3971819/886730326.py:3\n",
      ".\n",
      ".  6.603 <module>  ../../../tmp/ipykernel_3971819/886730326.py:1\n",
      ".  └─ 6.603 generate_data  ../../../tmp/ipykernel_3971819/653696646.py:1\n",
      ".     ├─ 6.272 HilightWiki.xent_sort  highlight.py:148\n",
      ".     │  ├─ 5.824 GPT2LMHeadModel._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      ".     │  │     [138 frames hidden]  torch, transformers, importlib, <buil...\n",
      ".     │  │        1.604 _VariableFunctionsClass.embedding  <built-in>\n",
      ".     │  ├─ 0.235 _VariableFunctionsClass.sort  <built-in>\n",
      ".     │  └─ 0.211 cross_entropy  torch/nn/functional.py:3014\n",
      ".     │        [1 frames hidden]  <built-in>\n",
      ".     ├─ 0.206 HilightWiki.extract  highlight.py:125\n",
      ".     │  ├─ 0.115 WikiArticle.__init__  highlight.py:80\n",
      ".     │  │  └─ 0.075 WikiArticle.tokenization  highlight.py:92\n",
      ".     │  │     └─ 0.075 tokenize  highlight.py:26\n",
      ".     │  └─ 0.082 WikiArticle.get_training_sample  highlight.py:95\n",
      ".     │     └─ 0.081 _VariableFunctionsClass.cat  <built-in>\n",
      ".     └─ 0.097 detokenize_l  highlight.py:29\n",
      ".        └─ 0.093 GPT2TokenizerFast.decode  transformers/tokenization_utils_base.py:3986\n",
      ".              [2 frames hidden]  transformers\n",
      ".  \n",
      ".....................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyinstrument\n",
    "\n",
    "with pyinstrument.profile():\n",
    "    text = generate_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pstats\n",
    "# from pstats import SortKey\n",
    "# import cProfile\n",
    "# cProfile.run(\"generate_data()\", \"restats\")\n",
    "# p = pstats.Stats(\"restats\")\n",
    "# p.strip_dirs().sort_stats(SortKey.TIME).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = 1e5\n",
    "# separator = \"£££$$$£££\"\n",
    "\n",
    "# for i in tqdm(range(int(gen))):\n",
    "#     output = generate_data()\n",
    "#     with open(\"highlighting_data.txt\", \"a\") as f:\n",
    "#         f.write(output + separator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
