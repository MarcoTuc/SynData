{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c257f520364d77841ff405f2f8b31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "work_dir = os.path.join(home_dir, \"synth\", \"highlighting\")\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML AND SCI LIBRARIES\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# XENT code\n",
    "from xentlang import X\n",
    "from utils import Tee\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "models_path = os.path.join(work_dir, \"models\")\n",
    "data_path = os.path.join(work_dir, \"data\")\n",
    "\n",
    "# utility parameters\n",
    "cut_dataset = None\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 6e-4 # take it from Karpathy nano-GPT \n",
    "EPOCHS = 15\n",
    "# TODO add all the available hyperparameters\n",
    "data_split = 0.6 # train/test ratio\n",
    "\n",
    "beta1 = 0.1\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(path: str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, clean_up_tokenization_spaces=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model(path: str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "    return model\n",
    "\n",
    "def load_torch_model(path: str):\n",
    "    model = torch.load(path, weights_only=False)\n",
    "    return model \n",
    "\n",
    "def load_tokenizer(path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, clean_up_tokenization_spaces=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "# DATA LOADING METHOD\n",
    "def load_dataset(name: str):\n",
    "    with open(os.path.join(data_path, f\"{name}.pkl\"), \"rb\") as data:\n",
    "        return pickle.load(data)\n",
    "    \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset: list[str], tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.dataset = [self.tokenize(text) for text in tqdm(dataset)]\n",
    "\n",
    "    def tokenize(self, text): \n",
    "        return self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        ).to(device)\n",
    "    \n",
    "    def tokenize_single(self, text):\n",
    "        return self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index) -> str:\n",
    "        return self.dataset[index]\n",
    "\n",
    "def find_xent_def(tokens):\n",
    "    \"\"\" Returns the index at which the xent function starts, needed for starting the loss computation \"\"\"\n",
    "    xdefseq = tokenizer.encode(X.xdef, return_tensors=\"pt\").to(device)\n",
    "    seq_len = xdefseq.shape[1]\n",
    "    windows = tokens.input_ids.unfold(dimension=2, size=seq_len, step=1)\n",
    "    matches = (windows==xdefseq).all(dim=3)\n",
    "    indices = matches.nonzero().squeeze(0)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load the model\n",
    "path = os.path.join(models_path, \"gpt2-xl-M0\")\n",
    "M0, tokenizer = load_model_and_tokenizer(path)\n",
    "def tokenize(text): return tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from highlight import WikiArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = WikiArticle(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2341 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21926, 26345, 40926,  ...,   262, 20572, 25219]], device='cuda:3'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:3')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(a.article[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
