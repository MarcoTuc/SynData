{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import model, tokenizer, device\n",
    "from synth import cross_entropy_given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Angelo told me that his favourite pizza is the quattroformaggi rossa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"{Y}\\n::INTRODUCE::\"\"\"\n",
    "input = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "outputs = torch.tensor([])\n",
    "for i in range(3):\n",
    "    torch.cuda.empty_cache()\n",
    "    output = model.generate(\n",
    "        input.input_ids,\n",
    "        attention_mask=input.attention_mask,\n",
    "        max_length=len(input.input_ids[0])+6,\n",
    "        num_return_sequences=1000,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=1\n",
    "    )\n",
    "    outputs = torch.cat((outputs, output.to(torch.device('cpu'))), dim=0)\n",
    "\n",
    "results = []\n",
    "for i in output:\n",
    "    answ = tokenizer.decode(i[len(input.input_ids[0]):], skip_special_tokens=True)\n",
    "    results.append((\n",
    "        cross_entropy_given(answ, Y, diff=True).item(),\n",
    "        answ\n",
    "        ))\n",
    "best_result = min(results, key=lambda x: x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
