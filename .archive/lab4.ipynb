{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from synth import CrossEntropyDifferential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "cross_entropy_differential = CrossEntropyDifferential(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n",
      "torch.Size([84, 50257])\n",
      "torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Angelo told me that his favourite pizza is the quattroformaggi rossa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"{Y}\\n::INTRODUCE::\"\"\"\n",
    "input = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "outputs = torch.tensor([])\n",
    "for i in range(3):\n",
    "    torch.cuda.empty_cache()\n",
    "    output = model.generate(\n",
    "        input.input_ids,\n",
    "        attention_mask=input.attention_mask,\n",
    "        max_length=len(input.input_ids[0])+6,\n",
    "        num_return_sequences=10,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=1\n",
    "    )\n",
    "    outputs = torch.cat((outputs, output.to(torch.device('cpu'))), dim=0)\n",
    "\n",
    "results = []\n",
    "for i in output:\n",
    "    answ = tokenizer.decode(i[len(input.input_ids[0]):], skip_special_tokens=True)\n",
    "    results.append((\n",
    "        cross_entropy_differential(answ, Y, diff=True).item(),\n",
    "        answ\n",
    "        ))\n",
    "best_result = min(results, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1382145881652832, '\\n*This is my favourite')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(best_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
