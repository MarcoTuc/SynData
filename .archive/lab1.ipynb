{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tuccio/synth/menv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making pizza is my favorite thing in the world. I love to cook it, and it's the only thing I can think of that I've ever really tried.\n",
      "\n",
      "I've always loved to make pizza with my friends, so I decided to\n"
     ]
    }
   ],
   "source": [
    "## Generation on a prompt \n",
    "\n",
    "prompt = \"Making pizza is my favorite\" \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "\n",
    "output = model.generate(\n",
    "    inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_length=50, \n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.5)\n",
    "\n",
    "generation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with highest probability:  sauce\n",
      "Token at position 4:  cheese\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Making pizza is my favorite thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. My Italian friend Giovanni told me that his favourite pizza is the capricciosa with salsiccia\" \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "logits = output.logits\n",
    "\n",
    "pick = 4\n",
    "sorted = torch.sort(logits[0][-1], 0, descending=True).indices\n",
    "tokenizer.decode(sorted[pick])\n",
    "max_token = tokenizer.decode(sorted[0])\n",
    "\n",
    "print(f\"Token with highest probability: {max_token}\")\n",
    "print(f\"Token at position {pick}: {tokenizer.decode(sorted[pick])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = \"Eating pizza is my favorite\"\n",
    "# X = \"Jesus was born 2024 years ago\"\n",
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Giovanni told me that his favourite pizza is the capricciosa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "Z = X + Y\n",
    "\n",
    "xt = tokenizer(X, return_tensors='pt', padding=True)\n",
    "yt = tokenizer(Y, return_tensors='pt', padding=True)\n",
    "type(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy 1: 3.231503963470459\n"
     ]
    }
   ],
   "source": [
    "X = \"Eating pizza is my favorite\"\n",
    "# X = \"Jesus was born 2024 years ago\"\n",
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Giovanni told me that his favourite pizza is the capricciosa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "Z = X + Y\n",
    "\n",
    "xt = tokenizer(X, return_tensors='pt', padding=True)\n",
    "yt = tokenizer(Y, return_tensors='pt', padding=True)\n",
    "zt = tokenizer(Z, return_tensors='pt', padding=True)\n",
    "\n",
    "len_xt = len(xt.input_ids[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_z = model(**zt)\n",
    "\n",
    "z_logits_tensor = output_z.logits\n",
    "\n",
    "cross_entropy = F.cross_entropy(z_logits_tensor[0, len_xt:][:-1], yt.input_ids[0][1:])\n",
    "\n",
    "print(f\"Cross Entropy 1: {cross_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy 1: 3.231503963470459 -- 6 tokens long\n",
      "Cross Entropy 2: 3.5444490909576416 -- 6 tokens long\n"
     ]
    }
   ],
   "source": [
    "X_1 = \"Eating pizza is my favorite\"\n",
    "X_2 = \"Jesus was born 2024 years ago\"\n",
    "# X_2 = \"My old ford focus is\"\n",
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Giovanni told me that his favourite pizza is the capricciosa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "Z_1 = X_1 + Y\n",
    "Z_2 = X_2 + Y\n",
    "\n",
    "xt_1 = tokenizer(X_1, return_tensors='pt', padding=True)\n",
    "xt_2 = tokenizer(X_2, return_tensors='pt', padding=True)\n",
    "yt = tokenizer(Y, return_tensors='pt', padding=True)\n",
    "zt_1 = tokenizer(Z_1, return_tensors='pt', padding=True)\n",
    "zt_2 = tokenizer(Z_2, return_tensors='pt', padding=True)\n",
    "\n",
    "len_xt_1 = len(xt_1.input_ids[0])\n",
    "len_xt_2 = len(xt_2.input_ids[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_z_1 = model(**zt_1)\n",
    "    output_z_2 = model(**zt_2)\n",
    "\n",
    "z_logits_tensor_1 = output_z_1.logits\n",
    "z_logits_tensor_2 = output_z_2.logits\n",
    "\n",
    "cross_entropy_1 = F.cross_entropy(z_logits_tensor_1[0, len_xt_1:][:-1], yt.input_ids[0][1:])\n",
    "cross_entropy_2 = F.cross_entropy(z_logits_tensor_2[0, len_xt_2:][:-1], yt.input_ids[0][1:])\n",
    "\n",
    "print(f\"Cross Entropy 1: {cross_entropy_1} -- {len_xt_1} tokens long\")\n",
    "print(f\"Cross Entropy 2: {cross_entropy_2} -- {len_xt_2} tokens long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.231503963470459 \t --  Eating pizza is my favorite\n",
      "3.5339298248291016 \t --  Dinosaurs arrived on earth\n",
      "-0.22489690780639648 \t --  Eating pizza is my favorite\n",
      "0.0775289535522461 \t --  Dinosaurs arrived on earth\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_given(X, Y, diff=False):\n",
    "\n",
    "    \"\"\" Compute the cross entropy of sentence Y given sentence X \"\"\"\n",
    "    \n",
    "    len_x = len(tokenizer(X, return_tensors='pt', padding=True).input_ids[0])\n",
    "    yt = tokenizer(Y, return_tensors='pt', padding=True)\n",
    "    zt = tokenizer(X + Y, return_tensors='pt', padding=True)\n",
    "    if diff: \n",
    "        with torch.no_grad():\n",
    "            output_z = model(**zt)\n",
    "            output_y = model(**yt)\n",
    "        logits_z = output_z.logits\n",
    "        logits_y = output_y.logits\n",
    "        cross_y = F.cross_entropy(logits_y[0][:-1], yt.input_ids[0][1:])\n",
    "        cross_z = F.cross_entropy(logits_z[0, len_x:][:-1], yt.input_ids[0][1:])\n",
    "        return cross_z - cross_y\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = model(**zt)\n",
    "        logits = output.logits\n",
    "        return F.cross_entropy(logits[0, len_x:][:-1], yt.input_ids[0][1:])\n",
    "\n",
    "\n",
    "X_1 = \"Eating pizza is my favorite\"\n",
    "# X_2 = \"I don't know what's the\"\n",
    "X_2 = \"Dinosaurs arrived on earth\"\n",
    "Y = \"\"\" thing in the world, all the mozzarella and the tomato sauce on my margherita are amazing. \n",
    "My Italian friend Giovanni told me that his favourite pizza is the capricciosa with salsiccia. \n",
    "When I was a kid, I used to watch the pizza maker create his pizzas, he was from Romania but a very nice gentleman I have to say.\"\"\"\n",
    "\n",
    "print(cross_entropy_given(X_1, Y).item(),\"\\t -- \", X_1)\n",
    "print(cross_entropy_given(X_2, Y).item(),\"\\t -- \", X_2)\n",
    "\n",
    "print(cross_entropy_given(X_1, Y, diff=True).item(),\"\\t -- \", X_1)\n",
    "print(cross_entropy_given(X_2, Y, diff=True).item(),\"\\t -- \", X_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
