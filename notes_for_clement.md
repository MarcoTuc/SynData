It's taking only 3 seconds to generate a data point for the summarization task with my code. 
What should I measure while running the things? 

Datapoints are 80% just wikipedia text and 20% the function to predict.
Should I give more weight to the tokens relative to the function when computing the loss?
Or maybe even compute the loss only on those tokens? 


