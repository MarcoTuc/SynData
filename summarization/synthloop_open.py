import torch
import torch.nn.functional as F
import llama_cpp
from tqdm import tqdm 
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import pickle
from synth import CrossEntropyDifferential

# instantiating phi to generate suggestions
master = llama_cpp.Llama(
      model_path="models/Phi-3.5-mini-instruct.Q8_0.gguf",
      n_gpu_layers=-1, 
      verbose=False,
      logits_all=False,
)

# gpt2 to evaluate cross-entropy and train
device = torch.device("cuda")
model_name = "gpt2"
slave = AutoModelForCausalLM.from_pretrained(model_name).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)
tokenizer.pad_token = tokenizer.eos_token
slave.config.pad_token_id = slave.config.eos_token_id
cross_entropy_differential = CrossEntropyDifferential(slave, tokenizer, device)

def generate_Y_slave(
        model, 
        tokenizer, 
        seed="\n",
        how_many=1,
        max_new_tokens=16,
        temperature=1.2
    ):
    """ Returns a list of Y phrases generated by the slave model """
    input = tokenizer(seed, return_tensors='pt', padding=True).to(device)
    output = model.generate(
            **input,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            num_return_sequences=how_many,
            do_sample=True, # uncomment to enable fancy sampling of the output distribution
            no_repeat_ngram_size=2,
            pad_token_id=model.config.eos_token_id
        )
    return tokenizer.batch_decode(output, skip_special_tokens=True)

def generate_X(Y, max_tokens=30):
    S = "Write a TL;DR: of the following text in exactly 5 words. No more, no less. Separate words with spaces. End with a . after writing it."
    prompt = f"""<|system|>{S}<|end>
    <|user|>{Y}<|end|>
    <|assistant|>"""
    return master(prompt, temperature=4, stop=["."], max_tokens=max_tokens)["choices"][0]["text"]

def ced(X, Y):
    return cross_entropy_differential(X, Y, diff=True)


n_gen = 2000
n_xtrials = 1000

Ys = generate_Y_slave(slave, tokenizer, how_many=n_gen)
Syntheses = []

for Y in tqdm(Ys, "Phrases completed"):
    best = ("", 0)
    for _ in tqdm(range(n_xtrials), desc="X trials completed"):
        X = generate_X(Y, max_tokens=20)
        rank = ced(X, Y)
        if rank < best[1]:
            best = (X, rank)
    Syntheses.append({f"{Y}::TLDR::{X}": rank.item()})

with open("new_syntheses.pkl", "wb") as f:
    pickle.dump(Syntheses, f)